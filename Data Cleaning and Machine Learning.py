# -*- coding: utf-8 -*-
"""Assi1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19YVrn-kKG86gJmubuNEh8aV8AABtwkpM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import nltk
import re as re
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk import word_tokenize,sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA

#stopwords.words("english")[:10] # <-- import the english stopwords

df1 = pd.read_csv('/content/drive/MyDrive/fashion dataset.csv')
df2 = pd.read_csv('/content/drive/MyDrive/fashion brand details.csv')

df1.head()

df2.head()

df1.rename(columns={'brand':'brand_name'}, inplace=True)
df1.head()

df1['brand_name'] = df1['brand_name'].str.lower()
df1.head()

df2['brand_name'] = df2['brand_name'].str.lower()
df2

df3= pd.merge(df1,df2,on="brand_name", how='outer')
df3.head()

df3.tail()

df3.columns = map(str.title, df3.columns)
df3.head()

df3.columns

df3.shape

df3.info()

df3[["P_Id", "Ratingcount", "Brand_Id"]] = df3[["P_Id", "Ratingcount", "Brand_Id"]].astype("Int64")
df3.info()

df3.describe().T

df3['Brand_Name'] = df3['Brand_Name'].str.title()
df3.head()

df3 = df3[["Brand_Id","P_Id","Name","Price","Colour","Brand_Name","Ratingcount","Avg_Rating","Description","P_Attributes"]]
df3.head()

df3['Avg_Rating']= df3['Avg_Rating'].round(1)
df3.head()

df3.nunique()

df3.isnull()

df3.isnull().sum()

df3.duplicated().sum()

df3['P_Id'].duplicated().sum()

df3=df3.dropna(how='all')
df3

"""Drop row if it does not have at least 6 values that are not NaN"""

df3=df3.dropna(thresh=6)
df3.head()

df3= df3.drop_duplicates(subset=['P_Id'], keep='last')
df3

df3['P_Id'].duplicated().sum()

df3.isnull().sum()

df3=df3.dropna(subset = ['Brand_Id']).dropna(subset = ['Colour'])
df3

df3=df3.fillna(0)
df3

df3.isnull().sum()

def preprocess_text(text: str, remove_stopwords: bool) -> str:
    """This utility function sanitizes a string by:
    - removing links
    - removing special characters
    - removing numbers
    - removing stopwords
    - transforming in lowercase
    - removing excessive whitespaces
    Args:
        text (str): the input text you want to clean
        remove_stopwords (bool): whether or not to remove stopwords
    Returns:
        str: the cleaned text
    """

    # remove links
    text = re.sub('&nbsp;', ' ', text)
    text = re.sub('<br>', ' ', text)
    text = re.sub('<[^<]+?>', '', text)
    text = re.sub(r"http\S+", "", text)
    # remove special chars and numbers
    text = re.sub("[^A-Za-z]+", " ", text)
    # remove stopwords
    if remove_stopwords:
        # 1. tokenize
        tokens = nltk.word_tokenize(text)
        # 2. check if stopword
        tokens = [w for w in tokens if not w.lower() in stopwords.words("english")]
        # 3. join back together
        text = " ".join(tokens)
    # return text in lower case and stripped of whitespaces
    text = text.lower().strip()
    return text

df3['Description']

df3['Description'] = df3['Description'].apply(lambda x: preprocess_text(x, remove_stopwords=True))
df3['Description']

df3["P_Attributes"]=df3["P_Attributes"].apply(eval)

df3["P_Attributes"]

df4=pd.json_normalize(df3["P_Attributes"])
df4



df4=df4.dropna(thresh=df4.shape[0]*0.55,axis=1)
df4

df=df3.join(df4)
df.head()

df = df.drop('P_Attributes', axis=1)
df.head()

df= df.dropna(thresh=16)
df

df = df.fillna('NA')
df

df.head()



from google.colab import files
df.to_csv('Final Dataset1.csv', encoding = 'utf-8-sig', index=False) 
files.download('Final Dataset1.csv')

df['Brand_Name'].value_counts().head(20)

df['Colour'].value_counts()

plt.figure(figsize = (12, 6))
sns.countplot(x = (df['Occasion']))
xt = plt.xticks(rotation=45)

sns.set(rc={'figure.figsize':(16,9)})

g = df.groupby('Brand_Name', as_index=False)['Ratingcount'].sum().sort_values(by='Ratingcount', ascending=False).head(10)
sns.barplot(data=g, x='Brand_Name', y='Ratingcount', hue='Brand_Name', dodge=False).set(xticklabels=[]);

data=df[['Brand_Name','Ratingcount','Price']]
sns.pairplot(data)

df.corr()

# Increase the size of the heatmap.
plt.figure(figsize=(12, 6))
heatmap = sns.heatmap(df.corr(),cmap="PiYG", vmin=-1, vmax=1, annot=True)
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);



"""
Machine Learning"""

#d1= df['Description'].values
# initialize the vectorizer
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X
X = vectorizer.fit_transform(df['Description'])

X

wcss=[]
for i in range(1,8):
    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.plot(range(1,8),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# initialize kmeans with centroids
k=3
kmeans = KMeans(n_clusters=k, random_state=42)
# fit the model
kmeans.fit(X)
# store cluster labels in a variable
clusters = kmeans.labels_

clusters

"""This code returns the keywords for each centroid of the KMeans"""
print("Cluster centroids: \n")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()

for i in range(k):
    print("Cluster %d:" % i)
    for j in order_centroids[i, :10]: #print out 10 feature terms of each cluster
        print (' %s' % terms[j])
    print('------------')

# initialize PCA with 2 components
pca = PCA(n_components=2, random_state=42)
# pass our X to the pca and store the reduced vectors into pca_vecs
pca_vecs = pca.fit_transform(X.toarray())
# save our two dimensions into x0 and x1
x0 = pca_vecs[:, 0]
x1 = pca_vecs[:, 1]

df['cluster'] = clusters
df['x0'] = x0
df['x1'] = x1

# map clusters to appropriate labels 
cluster_map = {0: "Cloth_Name", 1: "Measurement", 2: "Cloth_Type"}
# apply mapping
df['cluster'] = df['cluster'].map(cluster_map)

# set image size
plt.figure(figsize=(15, 10))
# set a title
plt.title("TF-IDF + KMeans clustering", fontdict={"fontsize": 18})
# set axes names
plt.xlabel("X0", fontdict={"fontsize": 16})
plt.ylabel("X1", fontdict={"fontsize": 16})
# create scatter plot with seaborn, where hue is the class used to group the data
sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette="viridis")
plt.show()

#plt.scatter(df['Price'], df['Avg_Rating'])Avg_Rating
sns.scatterplot(x=df['Avg_Rating'], y=df['Price'])

Y = df[['Price','Avg_Rating']].values
Y

wcss=[]
for i in range(1,11):
    kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0)
    kmeans.fit(Y)
    wcss.append(kmeans.inertia_)

plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

range_n_clusters = [2, 3, 4, 5, 6]
for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7) 
    ax1.set_xlim([-0.1, 1])
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(Y) + (n_clusters + 1) * 10])
    # Initialize the clusterer with n_clusters value and a random generator
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(Y)
    # clusters
    silhouette_avg = silhouette_score(Y, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(Y, cluster_labels)

    y_lower = 10
    for i in range(n_clusters): 
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(Y[:, 0], Y[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on Price and Avg_Rating data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()





